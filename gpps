#!/usr/bin/python
# Compute the tree which maximizes the likehood
from gurobipy import *
import sys, os
import math
import errno
from datetime import datetime
import argparse
import itertools
from collections import namedtuple
import collections
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt 
from operator import itemgetter
#user-created libraries
sys.path.append(os.path.abspath(os.getcwd() + '/utils'))
from matrix_utils import *
from outputs import *


#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!COSE DA FARE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#TROVARE/PROVARE AD UTILIZZARE LE PERCENTUALI LOGARITMICHE
#PROBLEMA QUANDO CANCELLO TROPPE ROW/COL E MI RITROVO DELLE NUOVE ROW/COL CHE SODDISFANO NUOVAMENTE I CRITERI
#UTILIZZARE STESSO APPROCCIO CON LE MUTAZIONI
#TROVARE UNA LIBRERIA SU NETWORKX PER IL MERGE DEI NODI

#devo risolvere il problema che mi da' quando lancio il file 2.txt


#create graph for rows and return it
def create_graph(nrow,d,p):
    G = nx.Graph()
    G.add_nodes_from(range(nrow))
    #'''
    'control view'
    plt.subplot(221)
    nx.draw(G, with_labels=True, font_weight='bold')
    #'''
    [G.add_edge(key[0],key[1], weight = value) for key, value in d.iteritems() if value>p]
    #'''
    plt.subplot(222)
    nx.draw(G, with_labels=True, font_weight='bold')
    plt.show()
    #'''
    return G


#for every pair rows returns likelihood probability
def prob_mergin(row1, row2, alpha, beta):
    prob = 1
    lenght = len(row1)
    for i in range(lenght):
        if row1[i] == row2[i]:
            if row1[i] == 1:
                prob = prob * ((1-beta)**2+beta**2)
            elif row1[i] == 0:
                prob = prob * ((1-alpha)**2+alpha**2)
        elif (row1[i] != 2) and (row2[i] != 2):
            prob = prob * ((1-beta)*alpha + (1-alpha)*beta)
    return prob

#function support to merge_rows !!!the miss_value are changed!!!
def merge_cycle_clone(matrix,cycle):
    print "cycle for row:",cycle 
    clones = matrix[np.array(cycle)]
    new_clone = []
    for i in range(len(clones[0])):
        col = collections.Counter(clones[:,i])
        if col[0] >= col[1]:
            new_clone = np.append(new_clone, 0)
        else:
            new_clone = np.append(new_clone, 1)
    for index in cycle:
        matrix[index] = new_clone
    print "new clone:",new_clone
    print "matrix now",matrix     
    return matrix  

##function support to merge_rows, it search the max edge with clone !!!the miss_value are changed!!! 
def find_max_edge(graph,matrix,index):
    edge_max_similar = sorted(G.edges(index,data=True),key= lambda x: x[2]['weight'],reverse=True)[0]    
    matrix[index] = matrix[edge_max_similar[1]]
    return matrix
    
#merge rows with higher likelihood value, behind merge the cicle, after merge the single edge, if there are
def merge_rows(matrix, graph):
    check = np.zeros((1,len(list(G.node))))
    cycles = sorted(map(sorted,nx.cycle_basis(G)))
    if len(cycles) != 0:
        edge_max_value = sorted(G.edges(data=True),key= lambda x: x[2]['weight']<1,reverse=True)[0]
        c = max([l for l in cycles if edge_max_value[1] in l and edge_max_value[0] in l],key=len)
        cycle = cycles.pop(cycles.index(sorted(list(c))))
        for j in cycle:
            check[0][j] = 1
        matrix = merge_cycle_clone(matrix, cycle)
        for i in range(len(cycles)):
            indexes = []
            for index in cycles[i]:
                if check[0][index] == 0:
                    indexes.append(index)
                    check[0][index] == 1
            if len(indexes) == 1:
                matrix = find_max_edge(graph,matrix,indexes[0])
            else:
                matrix = merge_cycle_clone(matrix, indexes)   
    edges_isolated = [list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True) if len(c)==2]
    if len(edges_isolated) != 0:
        for edge in edges_isolated:
            if check[0][edge[0]] == 1:
                matrix = find_max_edge(graph,matrix,edge[1]) 
            elif check[0][edge[1]] == 1:
                matrix = find_max_edge(graph,matrix,edge[0]) 
            else:
                matrix = merge_cycle_clone(matrix,edge)
    return matrix

#for every pair cols(mutations) returns likelihood probability
def prob_merging_mutations(col1,col2,alpha,beta):
	lenght = len(col1)
	prob = 1
	for i in range(1,lenght):
		if col1[i] == col2[i]:
			if col1[i] == 1:
				prob = prob * ((1-alpha)**2+alpha**2)
			elif col1[i] == 0:
				prob = prob * ((1-beta)**2+beta**2)
		elif (col1[i]!=2) and (col2[i]!=2):
			prob = prob * ((1-alpha)*beta + (1-beta)*alpha)
	return prob


#function support to merge_cols !!!the miss_value are changed!!!
def merge_cycle_mutation(matrix,cycle):
    print "cycle for col:",cycle
    mutations = matrix[:,cycle]
    new_mutation = []
    for i in range(len(mutations)):
        row = collections.Counter(mutations[i])
        if row[0] >= row[1]:
            new_mutation = np.append(new_mutation, 0)
        else:
            new_mutation = np.append(new_mutation, 1)
    for index in cycle:
        matrix[:,index] = new_mutation
    print "new mutation:",new_mutation

    return matrix


#merge cols with higher likelihood value
def merge_cols(matrix, d, percent):
    check = np.zeros((1,len(matrix[0])))
    G = nx.Graph()
    [G.add_edge(key[0],key[1], weight = value) for key, value in d.iteritems() if value>percent]
    cycles = sorted(map(sorted,nx.cycle_basis(G)))
    if len(cycles) != 0:
        edge_max_value = sorted(G.edges(data=True),key= lambda x: x[2]['weight']<1,reverse=True)[0]
        c = max([l for l in cycles if edge_max_value[1] in l and edge_max_value[0] in l],key=len)
        cycle = cycles.pop(cycles.index(sorted(list(c))))
        for j in cycle:
            check[0][j] = 1
        matrix = merge_cycle_mutation(matrix, cycle)
        for i in range(len(cycles)):
            indexes = []
            for index in cycles[i]:
                if check[0][index] == 0:
                    indexes.append(index)
                    check[0][index] == 1
            if len(indexes) == 1:
                matrix = find_max_edge(G,matrix,indexes[0])
            else:
                matrix = merge_cycle_mutation(matrix, indexes)
    return matrix

#merge clones/rows with many missing value
def remove_clones(matrix,missing_value):
    lenght = len(matrix[0])
    index = []
    for i in range(len(matrix)):
        occurrences = list(matrix[i]).count(2)
        if float(occurrences)/lenght>missing_value:#float(occurrences)/lenght>math.exp(missing_value):
            index.append(i)
    for num in index:		
        matrix = np.delete(matrix, num, 0)
    return matrix

#delete mutation/cols with many missing value or with few value 1
def remove_mutations(matrix, missing_value, few_one):
    lenght = len(matrix[0])
    index = []
    for i in range(lenght):
        occurrences_data_missing = list(matrix[:,i]).count(2)
        occurrences_few_mutation = list(matrix[:,i]).count(1)
        if float(occurrences_data_missing)/len(matrix)>missing_value or float(occurrences_few_mutation)/len(matrix)<few_one:#add math.exp() at missing_value and few_one for percentage logarithmic scale
            index.append(i)
    for num in index[::-1]:
	    matrix = np.delete(matrix, num, 1)
    return matrix

#return if the mutations are independent or dependent with probability
def prob_independent_mutations(col1,col2,alpha,beta):
    lenght = len(col1)
    prob_ind = 1
    prob_not_independent = 1
    for i in range(lenght):
        if col1[i] == col2[i]:
            if col1[i] == 1:
                prob_ind = prob_ind * (1-((1-beta)**2))
                prob_not_independent = prob_not_independent * (((1-beta)**2))
            elif col1[i] == 0:
                prob_ind = prob_ind * ((1-alpha)**2)
                prob_not_independent = prob_not_independent * (1-((1-alpha)**2))
        elif (col1[i] != 2) and (col2[i] != 2):
            prob_ind = prob_ind * (1-((1-beta)*alpha))
            prob_not_independent = prob_not_independent * (((1-beta)*alpha))
    #print "prob_not_independent", prob_not_independent
    #print "prob_ind", prob_ind
    return prob_ind, prob_not_independent


#==================================================================#
#========================= PREREQUISITES ==========================#
#==================================================================#

#--------------------------Parse Arguments-------------------------#
parser = argparse.ArgumentParser(description='gpps', add_help=True)

parser.add_argument('-f', '--file', action='store', type=str, required=True,
                    help='path of the input file.')
parser.add_argument('-k', action='store', type=int,
                    help='k-value of the selected model. Eg: Dollo(k)')
parser.add_argument('-t', '--time', action='store', type=int, required=True,
                    help='maximum time allowed for the computation. Type 0 to not impose a limit.')
parser.add_argument('-o', '--outdir', action='store', type=str, required=True,
                    help='output directory.')

parser.add_argument('-e', '--exp', action='store_true', default=False,
                    help='set -e to get experimental-format results.')
parser.add_argument('-b', '--falsepositive', action='store', type=float, required=True,
                    help='set -b False positive probability.')
parser.add_argument('-a', '--falsenegative', action='store', type=float, required=True,
                    help='set -a False negative probability.')
parser.add_argument('-p', '--percent', action='store', type=float, required=True,
					help='set -p accuracy threshold probability for clones or mutations.')
parser.add_argument('-mv', '--missingvalues', action='store', type=float, required=True,
					help='set -mv max missing values for every row or col,in percent')
parser.add_argument('-fo', '--fewone', action='store', type=float, required=True,
					help='set -fp threshold for mutation with few one,in percent')
parser.add_argument('--mps', action='store_true',
                    help='This will output the model in MPS format instead of running the solver')

args = parser.parse_args()


max_gains = 1
max_losses = int(args.k)
alpha = float(args.falsenegative)
lalpha = math.log(alpha)
beta = float(args.falsepositive)
lbeta = math.log(beta)
percent = float(args.percent)
lpercent = math.log(percent)
missing_value = float(args.missingvalues)#math.log(float(args.missingvalues))
few_one = float(args.fewone)#math.log(float(args.fewone)) 

#----------------------Initialize program----------------------#
input_matrix = read_matrix_tab(args.file)
matrix_name = os.path.basename(args.file).split('.')[0]
input_matrix = np.array(input_matrix) #I exploit library NumPy

num_clones = len(input_matrix)
num_mutations = len(input_matrix[0])
print('parameters before the selection criteria')
print('Num mutations: %d' % num_mutations)
print('Num clones: %d' % num_clones)

print "original matrix:",input_matrix

input_matrix = remove_clones(input_matrix, missing_value)
input_matrix = remove_mutations(input_matrix, missing_value, few_one)

print "after missing-value and Few-one","\n",input_matrix

#I use the dict, pair clones(them index)and similar probability.
#make tuple 
tup_row = namedtuple('Tup',['index','percent'])
pair_percent =[]
for index1 in range(len(input_matrix)-1):
    for index2 in range(index1+1,len(input_matrix)):
        prob = prob_mergin(input_matrix[index1],input_matrix[index2],alpha,beta)
        pair_percent.append(tup_row((index1,index2),prob))

d_row = dict(pair_percent)
G = create_graph(len(input_matrix),d_row,percent)
input_matrix = merge_rows(input_matrix, G)

print "matrix after merge_rows:",input_matrix

#I use the dict, pair mutations(them index)and similar probability.
#make tuple
tup_col = namedtuple('Tup',['index','percent'])
pair_probability = []
#print input_matrix
for index1 in range(len(input_matrix[0])-1):
    for index2 in range(index1+1,len(input_matrix[0])):
        prob = prob_merging_mutations(input_matrix[:,index1],input_matrix[:,index2],alpha,beta)
        pair_probability.append(tup_col((index1,index2),prob))

d_col = dict(pair_probability)
input_matrix = merge_cols(input_matrix, d_col, percent)

print "matrix after merge_cols:",input_matrix

# Fixed parameters
#num_samples = len(input_matrix)
# num_clones = int(num_mutations * args.clones)
num_clones = len(input_matrix)
num_mutations = len(input_matrix[0])
num_columns = num_mutations * (1 + max_losses)
max_error = 1

print("parameters after the selection criteria")
#print('Num samples: %d' % num_samples)
print('Num mutations: %d' % num_mutations)
print('Num clones: %d' % num_clones)
#print input_matrix

pair_mutations_dependent = []
for index1 in range(num_mutations-1):
	for index2 in range(index1+1,num_mutations):
		prob_indipendent ,prob_not_independent = prob_independent_mutations(input_matrix[:,index1],input_matrix[:,index2],alpha,beta)
		pair_mutations_dependent.append([(index1,index2),prob])
#print "pair_mutations_dependent", pair_mutations_dependent

import os
try:
    os.makedirs(args.outdir)
except OSError as exc:
    if exc.errno == errno.EEXIST and os.path.isdir(args.outdir):
        pass
    else:
        raise

filename = os.path.splitext(os.path.basename(args.file))[0]
outfile = os.path.join(args.outdir, filename)

#==================================================================#
#========================== GUROBI MODEL ==========================#
#==================================================================#
start_model_time = datetime.now()
model = Model('Parsimony Phylogeny Model')
model.setParam('Threads', 4)
if args.time != 0:
    model.setParam('TimeLimit', args.time)

#---------------------------------------------------#
#------------------- VARIABLES ---------------------#
#---------------------------------------------------#

#-----------Variable Y and B---------------

#lalpha = math.log(alpha)
#lbeta = math.log(beta)
l_alpha = math.log(1-alpha)
l_beta = math.log(1-beta)
n_ones = 0
n_zeros = 0

print(lalpha, lbeta, l_alpha, l_beta)


print('Generating variables I, F, w, P.')
I = {}
F = {}
fminus = {}
P = {}
# False positive should be only a few
FP = {}

for row_index, row in enumerate(input_matrix):
    I[row_index] = {}
    F[row_index] = {}
    fminus[row_index] = {}
    P[row_index] = {}
    FP[row_index] = {}
    for col_index, cell in enumerate(row):
        # P
        names = expand_name(str(col_index), 1, max_losses)
        for name in names:
            P[row_index][name] = model.addVar(vtype=GRB.BINARY, obj=0,
                                              name='P{0}-{1}'.format(row_index, name))
        if cell < 2:
            # I
            I[row_index][col_index] = model.addVar(vtype=GRB.BINARY, obj=0,
                                                   name='I{0}-{1}'.format(row_index, col_index))
            model.update()
            model.addConstr(I[row_index][col_index] == cell,
                            "constr_I{0}-{1}".format(row_index, col_index))
            # F and fminus. fminus is equal to 1-F
            if cell == 0:
                F[row_index][col_index] = model.addVar(vtype=GRB.BINARY, obj=lalpha,
                                                       name='F{0}-{1}'.format(row_index, col_index))
                fminus[row_index][col_index] = model.addVar(vtype=GRB.BINARY, obj=l_beta,
                                                            name='f{0}-{1}'.format(row_index, col_index))
            if cell == 1:
                F[row_index][col_index] = model.addVar(vtype=GRB.BINARY, obj=l_alpha,
                                                       name='F{0}-{1}'.format(row_index, col_index))
                fminus[row_index][col_index] = model.addVar(vtype=GRB.BINARY, obj=lbeta,
                                                            name='f{0}-{1}'.format(row_index, col_index))
                FP[row_index][col_index] = model.addVar(vtype=GRB.BINARY, obj=0,
                                                        name='FP{0}-{1}'.format(row_index, col_index))
                model.update()
                model.addConstr(FP[row_index][col_index] == 1 - P[row_index][names[0]],
                                name='constr_FP{0}-{1}'.format(row_index, col_index))
            model.update()
            model.addConstr(F[row_index][col_index] == 1 - fminus[row_index][col_index],
                            name='constr_def_fminus{0}-{1}'.format(row_index, col_index))
            model.addConstr(F[row_index][col_index] == P[row_index][names[0]] - quicksum(P[row_index][name] for name in names[1:]),
                            name='constr_balance_F{0}-{1}'.format(row_index, col_index))
            model.addConstr(P[row_index][names[0]] >= quicksum(P[row_index][name] for name in names[1:]),
                            name='constr_imbalance_P{0}-{1}'.format(row_index, col_index))

# There are only a few false positives
model.addConstr(4 >= quicksum(FP[r][c] for r in FP.keys() for c in FP[r].keys()),
                name='constr_few_FP')

model.update()

print('Generating variables B.')
B = {}
columns = list(itertools.chain.from_iterable(
    [expand_name(str(name), 1, max_losses) for name in range(num_mutations)]))
for p in columns:
    B[p] = {}
for p, q in itertools.combinations(columns, 2):
    B[p][q] = {}
    B[p][q]['01'] = model.addVar(vtype=GRB.BINARY, obj=0,
                                 name='B[{0},{1},0,1]'.format(p, q))
    B[p][q]['10'] = model.addVar(vtype=GRB.BINARY, obj=0,
                                 name='B[{0},{1},1,0]'.format(p, q))
    B[p][q]['11'] = model.addVar(vtype=GRB.BINARY, obj=0,
                                 name='B[{0},{1},1,1]'.format(p, q))
    model.update()
    for row_index, row in enumerate(input_matrix):
        model.addConstr(B[p][q]['01'] >= P[row_index][q] - P[row_index][p],
                        "constr_B01-{0}-{1}-{2}".format(p, q, row_index))
        model.addConstr(B[p][q]['10'] >= P[row_index][p] - P[row_index][q],
                        "constr_B10-{0}-{1}-{2}".format(p, q, row_index))
        model.addConstr(B[p][q]['11'] >= P[row_index][p]+P[row_index][q]-1,
                        "constr_B11-{0}-{1}-{2}".format(p, q, row_index))
    model.addConstr(B[p][q]['01'] + B[p][q]['10'] + B[p][q]['11'] <= 2,
                    "constr_sum_B{0}-{1}".format(p, q))


print("UPDATE")
model.update()

#-------------OBJECTIVE FUNCTION-----------

model.modelSense = GRB.MAXIMIZE
model.update()

#---------------------------------------------------#
#------------------ CONSTRAINTS --------------------#
#---------------------------------------------------#


#-------------Model Specific constraints------------


if args.mps:
    print("Writing the model")
    model.write(outfile + '.mps')
    sys.exit(0)


print('#----- GUROBI OPTIMIZATION ----#')
start_optimize = datetime.now()
model.optimize()

#==================================================================#
#======================= POST OPTIMIZATION ========================#
#==================================================================#

matrix = []
# print(P)
with open('{0}.ilp.extended.out'.format(outfile), 'w+') as file_out:
    for row_index, row in enumerate(input_matrix):
        row_out = []
        for col_index, cell in enumerate(row):
            names = expand_name(str(col_index), 1, max_losses)
            for name in names:
                row_out.append(int(float(P[row_index][name].X)))
        matrix.append(row_out)
        file_out.write(' '.join([str(x) for x in row_out]))
        file_out.write('\n')

if model.status == GRB.Status.OPTIMAL or model.status == GRB.Status.TIME_LIMIT:
    value = float(model.objVal) + n_zeros * l_beta + n_ones * lbeta
    with open('{0}.ilp.log'.format(outfile), 'w+') as file_out:
        file_out.write('Optimal likelihood: %f\n' % value)